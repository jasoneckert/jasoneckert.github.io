<?xml version="1.0"?>
<content collectionGUID="6CD99A98-03EA-45F6-9464-3BE6AB01739F">
  <lastEdited clientType="local-build-20200628" date="2020-06-28 12:32:15 +0000"/>
  <textBox id="generic-title-attributes" dynamic="no" visible="yes">
    <richText>&lt;b&gt;Microsoft ReFS vs Oracle ZFS - FIGHT&lt;/b&gt;</richText>
  </textBox>
  <textBox id="generic-datefield-attributes" dynamic="no" visible="yes">
    <richText>&lt;b&gt;Tuesday, February 25, 2014&lt;/b&gt;</richText>
  </textBox>
  <textBox id="generic-body-attributes" dynamic="no" visible="yes">
    <richText>&lt;b&gt;Last night, I gave a presentation at &lt;/b&gt;&lt;b&gt;WWITPRO (Waterloo Wellington IT Professional Users Group) &lt;/b&gt;&lt;b&gt;that involved 3 mini presentations (to keep it fresh):&#xD;1)  Windows Server 2012 in a nutshell (with cool demos)&#x2028;2)  What is a Transputer and how did it shape today&#x2019;s computing world?&#x2028;3)  Microsoft ReFS vs Oracle ZFS - FIGHT! (with cool demos)&#x2028;&#xD;The first two presentations don&#x2019;t lend themselves well to a blog post since it&#x2019;s mostly demos and show-and-tell.....but the third one does, so this post is going to summarize some of the stuff from that presentation.&#xD;&#xD;In the past, most IT admins have put their faith in systems and &lt;/b&gt;&lt;b&gt;SANs (Storage Area Networks)&lt;/b&gt;&lt;b&gt; that use &lt;/b&gt;&lt;b&gt;RAID (Redundant Array of Inexpensive Disks)&lt;/b&gt;&lt;b&gt; technology.&#xA0; RAID can be used to combine hard disks together into &lt;/b&gt;&lt;b&gt;simple volumes&lt;/b&gt;&lt;b&gt; (called RAID-0, or JBOD) that aren&#x2019;t fault tolerant, but can also be used to create &lt;/b&gt;&lt;b&gt;mirrored volumes&lt;/b&gt;&lt;b&gt; where both drives are identical in case one fails (called RAID-1), or &lt;/b&gt;&lt;b&gt;striped volumes with parity&lt;/b&gt;&lt;b&gt; where data is written across several disks with parity information that can be used to calculate the missing data if a drive fails (called RAID-5).&#xA0;&#xD;&#xA0;&#xD;The biggest problem with this approach is that it only protects against hard disk failure.&#xA0; What about the plethora of different problems that can corrupt data as it is actually written to the file system by the operating system itself?  These include:&#xD;&lt;/b&gt;&lt;b&gt;	&#x2022;	Accidental driver overwrites&#xD;	&#x2022;	Bit rot&#xD;	&#x2022;	Disk firmware bugs&#xD;	&#x2022;	Driver &amp; kernel buffer errors&#xD;	&#x2022;	Misdirected writes&#xD;	&#x2022;	Phantom writes&#xD;	&#x2022;	Silent data corruption&#x2028;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Both ReFS and ZFS solve these problems at the file system level!&#xA0; &#xD;More specifically, they both:&#xD;&lt;/b&gt;&lt;b&gt;	&#x2022;	Detect and repair data errors in real time&#xD;	&#x2022;	Accommodate volumes in the thousands of disks&#xD;	&#x2022;	Support huge numbers of files&#xD;	&#x2022;	Allow you to modify/resize volumes without taking the volume offline&#x2028;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Now let&#x2019;s take a more in-depth look at each one&#x2026;..starting with ZFS.&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Zettabyte File System (ZFS):&#xD;&lt;/b&gt;&lt;b&gt;	&#x2022;	Is a 128-bit filesystem.&#xD;	&#x2022;	Has a capacity of 256 quadrillion Zettabytes (1 Zettabyte = 1 billion Terabytes)&#xD;	&#x2022;	Has been around for a long time (first introduced in 2001, officially released in 2005)&#xD;	&#x2022;	Is multi-platform (Solaris, Mac OSX, Linux, FreeBSD, FreeNAS, and more)&#xD;	&#x2022;	Supports nearly all file system features (Deduplication, snapshots, cloning, compression, encryption, NFSv4, volume management, etc.)&#xD;	&#x2022;	Has checksums built into its entire fabric &#x2013; it&#x2019;s very obvious that ZFS was designed to implement checksums in a very efficient way at every level.&#x2028;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;ZFS Terminology:&#xD;&lt;/b&gt;&lt;b&gt;ZFS pool = bunch of physical disks (managed with the &lt;/b&gt;&lt;b&gt;zpool&lt;/b&gt;&lt;b&gt; command)&#xD;ZFS file systems = volumes created from ZFS pools (managed with the &lt;/b&gt;&lt;b&gt;zpool&lt;/b&gt;&lt;b&gt; and &lt;/b&gt;&lt;b&gt;zfs&lt;/b&gt;&lt;b&gt; commands)&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Creating ZFS volumes:&#xD;&lt;/b&gt;&lt;b&gt;I did this on an Oracle UltraSPARC server running Solaris 10, but you can pretty much use any operating system that has ZFS support installed.&#xA0; You can give ZFS anything to use &#x2013; you could give it a device file for a raw device, a partition, a volume, or even a file on a filesystem!&#xA0; For ease, I&#x2019;m going to create 3 simple files off of the root of my system to use:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; mkfile 256m /disk1&#xD;&#xA0; mkfile 256m /disk2&#xD;&#xA0; mkfile 256m /disk3&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;Next, I can create a simple ZFS volume from all three devices called lala, and automatically mount it to the /lala directory using a single command:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zpool create lala /disk1 /disk2 /disk3&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;I can then copy files to /lala or check out the details using the &lt;/b&gt;&lt;b&gt;zpool&lt;/b&gt;&lt;b&gt; command:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; cp /etc/hosts /lala&#xD;&#xA0; ls -lh /lala&#xD;&#xA0; zpool list&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;I can also remove the ZFS volume entirely and return to my original configuration:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zpool destroy lala&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;Similarly, to create a mirror called po using just /disk1 and /disk2, and mount it to the /po directory, I could use:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zpool create po mirror /disk1 /disk2&#xD;&#xA0; zpool list&#xD;&#xA0; cp /etc/hosts /lala&#xD;&#xA0; zpool status po&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;If I overwrite part of /disk1 (simulating corruption) using the &lt;/b&gt;&lt;b&gt;dd&lt;/b&gt;&lt;b&gt; command, I can use the &lt;/b&gt;&lt;b&gt;zpool scrub&lt;/b&gt;&lt;b&gt; command to check for the error and then detach /disk1 (the failed disk) and attach /disk3 (a new disk).&#xA0; Finally, I&#x2019;ll check out some more detailed I/O stats and remove the po volume:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; dd if=/dev/random of=/disk1 bs=512 count=1&#xD;&#xA0; zpool scrub po&#xD;&#xA0; zpool status&#xD;&#xA0; zpool detach po /disk1&#xD;&#xA0; zpool status po&#xD;&#xA0; zpool attach po /disk2 /disk3&#xD;&#xA0; zpool list&#xD;&#xA0; zpool status po&#xD;&#xA0; zpool iostat -v po&#xD;&#xA0; zpool destroy po&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;To do a striped volume with parity, you can use the keyword &lt;/b&gt;&lt;b&gt;raidz&lt;/b&gt;&lt;b&gt; alongside the &lt;/b&gt;&lt;b&gt;zpool&lt;/b&gt;&lt;b&gt; command.&#xA0; RAID-Z is like RAID-5, but more intelligent because it uses a variable-sized stripe (hence the new name, RAID-Z)!&#xA0; Like RAID-5, RAID-Z requires a minimum of 3 devices.&#xA0; You can also use &lt;/b&gt;&lt;b&gt;raidz2&lt;/b&gt;&lt;b&gt; (double parity like RAID-6, requires 4+ devices) and &lt;/b&gt;&lt;b&gt;raidz3&lt;/b&gt;&lt;b&gt; (triple partity, requires 5+ devices).&#xA0; Let&#x2019;s just create a regular RAID-Z volume called noonoo and check it out:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zpool create noonoo raidz /disk1 /disk2 /disk3&#xD;&#xA0; zpool status noonoo&#xD;&#xA0; zpool iostat -v noonoo&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;There is far more to ZFS than I&#x2019;ll discuss here &#x2013; you can resize/modify/whatever using various &lt;/b&gt;&lt;b&gt;zpool&lt;/b&gt;&lt;b&gt; commands.&#xA0; But what I&#x2019;ll leave you with for now is that most of the cool features of the ZFS filesystem can be managed using the &lt;/b&gt;&lt;b&gt;zfs&lt;/b&gt;&lt;b&gt; command.&#xA0; For example, I could create three subdirectories under the /noonoo filesystem for three users using the &lt;/b&gt;&lt;b&gt;zfs&lt;/b&gt;&lt;b&gt; command (which is better than simply using &lt;/b&gt;&lt;b&gt;mkdir&lt;/b&gt;&lt;b&gt; to create them because it tells ZFS to keeps track of them):&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zfs create noonoo/larry&#xD;&#xA0; zfs create noonoo/curly&#xD;&#xA0; zfs create noonoo/moe&#xD;&#xA0; zfs list&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;Now you could get a list of the properties for the /noonoo/larry subdirectory to see a list of things you could modify, or even set a quota for larry:&#xD;&lt;/b&gt;&lt;b&gt;&#xA0; zfs get all noonoo/larry&#xD;&#xA0; zfs set quota=10G noonoo/larry&#xD;&lt;/b&gt;&lt;b&gt;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Resilient File System (ReFS):&#xD;&lt;/b&gt;&lt;b&gt;	&#x2022;	Support Windows Server 2012+ / Windows 8+ only&#xD;	&#x2022;	Is a 64-bit filesystem&#xD;	&#x2022;	Has a capacity = 1 Yottabyte (=1024 Zettabytes)&#xD;	&#x2022;	Puts 64-bit checksums on all metadata&#xD;	&#x2022;	Puts 64-bit checksums on data (if integrity attribute set)&#xD;	&#x2022;	Has NO support for EFS, quotas, compression, 8.3 names, deduplication, etc. (Bitlocker is supported though)&#x2028;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;ReFS Terminology:&#xD;&lt;/b&gt;&lt;b&gt;Storage pool = a bunch of physical disks&#xD;Storage spaces = virtual disks / volumes made from storage pools (can be NTFS or ReFS)&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Creating ReFS Volumes:&#xD;&lt;/b&gt;&lt;b&gt;ReFS works with Storage Spaces to create either simple volumes from one or more disks, mirrored volumes (RAID-1, requires 2 devices), or parity volumes (RAID-5, requires 3+ devices).&#xA0; ReFS automatically corrects file corruption on a parity space if the the integrity attribute is set (it is set by default on parity volumes, but can be changed using the Set-FileIntegrity cmdlet in PowerShell).&#xD;&#xA0;&#xD;Unfortunately, you can&#x2019;t just create a file and give it to Storage Spaces for creating ReFS volumes &#x2013; you actually have to use real devices.&#xA0; As a result, I&#x2019;ve installed Windows Server 2012&#xA0;R2 in a Hyper-V virtual machine and attached 3 additional 5GB virtual disks to it.&#xD;&#xD;You can easily create a storage pool within Server Manager by navigating to &lt;/b&gt;&lt;b&gt;File and Storage Services &gt; Volumes &gt; Storage Spaces&lt;/b&gt;&lt;b&gt;.&#xA0; When you choose the appropriate Task option to create a Storage Space, you&#x2019;ll get a wizard that asks you to choose the disks you want to put in the pool - you can then give it a name and complete the wizard.&#xA0; At this point, you&#x2019;ll have something that looks like this:&#xD;&#xD;&#xD;Next, you&#x2019;ll want to choose the &lt;/b&gt;&lt;b&gt;Tasks&lt;/b&gt;&lt;b&gt; menu in the Virtual Disks section shown above and create a new Storage Space volume.&#xA0; This opens two wizards.&#xA0; The first wizard asks you to choose the pool you want to use, and then give the Storage Space a name, layout (Simple, Mirror or Parity) and size.&#xA0; The second wizard creates the filesystem on the Storage Space - it prompts you to choose your Storage Space, volume size, mount point (e.g. F:\), and file system (choose ReFS!!!).&#xA0; And BOOM!&#xA0; You&#x2019;re done.&#xA0; You should have a new volume listed under Virtual Disks:&#xD;&#xD;&#xD;OK, now for the performance comparison!&#xA0; I&#x2019;ve had the pleasure of being able to run both file systems in a production-class test environment, and bog them down using various software suites.&#xA0; Here is what I&#x2019;ve found from my experience:&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;ReFS Performance:&#xD;&lt;/b&gt;&lt;b&gt;To say it is poor, would be very kind.&#xA0; The parity (RAID-5) has dismal performance, especially if you simulate corruption.&#xA0; Microsoft&#x2019;s only solution to this is to add an SSD (it will move frequently-access stuff to the SSD &#x2013; a feature called storage tier support).&#xA0; Why is this?!?!?&#xA0; Well, according to Microsoft, ReFS requires that existing parity information "...be read and processed before a new write can occur.&#x201D;&#xA0; Archaic to say the least.&#xD;&#xA0;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;ZFS Performance:&#xD;&lt;/b&gt;&lt;b&gt;Amazing would be an understatement.&#xA0; You throw anything at ZFS, and it seems to use it in the most efficent way possible (including PCIe SSDs and battery-backed RAM disk devices with ultra-low latency).&#xA0; There are also 3 levels of caching available with ZFS:&#xD;&lt;/b&gt;&lt;b&gt;	&#x2022;	ARC (Adaptive Replacement Cache), which is essentially intelligent memory caching&#xD;	&#x2022;	L2ARC, which is non-storage SSD read caching&#xD;	&#x2022;	ZIL (ZFS intent log), which is SSD write caching&#x2028;&#xD;&lt;/b&gt;&lt;b&gt;More importantly, ZFS writes parity info with a variable-sized stripe.&#xA0; This feature almost entirely eliminates poor performance, AND corrects the infamous RAID 5 Hole (data lost in the event of a power failure).&#xD;&#xA0;&#xD;&lt;/b&gt;&lt;b&gt;Conclusion:&#xD;&lt;/b&gt;&lt;b&gt;Both ZFS and ReFS detect and repair data errors without dismounting volumes, but ZFS does a much better job.&#xA0; ZFS uses checksums that trickle up through all data and metadata in the file system hierarchy.&#xA0; ReFS approximates this using the integrity attribute, but it has a huge hit on performance and very high latency.&#xA0;&#xA0; ZFS also supports more file system features and storage technologies (deduplication, PCI-e SSDs, Pro RAMdisks, compression, cloning, snapshots, encryption, and more).&#xA0;&#xD;&#xA0;&#xD;Consequently, &lt;/b&gt;&lt;b&gt;ZFS wins hands down&lt;/b&gt;&lt;b&gt;.&#xA0; However, don&#x2019;t write ReFS off just yet.&#xA0; Keep in mind that ZFS is very mature, and currently in production in massive enterprise environments.&#xA0; ReFS has just been introduced by Microsoft &#x2013; it&#x2019;s essentially a &#x201C;1.0&#x201D; feature that is rough around the edges.&#xA0; The important thing is that Microsoft wants a file system like ZFS, and it looks like they are moving in the right direction (which is a good thing).&lt;/b&gt;</richText>
  </textBox>
  <image id="generic-picture-attributes" dynamic="no" visible="yes" src="25_Microsoft_ReFS_vs_Oracle_ZFS_-_FIGHT_files/shapeimage_1.png" left="0px" top="0px" width="700px" height="400px"/>
</content>
